{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76eb025b",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Assume that there are 10 quanta available in a nerve terminal, and for a given release event each is released with a probability of 0.2. For one such event, what is the probability that 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, or 10 quanta will be released?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f5c3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 quanta released: 0.1073741824 (10.7374%)\n",
      "1 quanta released: 0.2684354560 (26.8435%)\n",
      "2 quanta released: 0.3019898880 (30.1990%)\n",
      "3 quanta released: 0.2013265920 (20.1327%)\n",
      "4 quanta released: 0.0880803840 (8.8080%)\n",
      "5 quanta released: 0.0264241152 (2.6424%)\n",
      "6 quanta released: 0.0055050240 (0.5505%)\n",
      "7 quanta released: 0.0007864320 (0.0786%)\n",
      "8 quanta released: 0.0000737280 (0.0074%)\n",
      "9 quanta released: 0.0000040960 (0.0004%)\n",
      "10 quanta released: 0.0000001024 (0.0000%)\n",
      "\n",
      "Sum of probabilities = 1.0000000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.stats as st\n",
    "import scipy.special as sp\n",
    "\n",
    "from scipy.stats import bernoulli, binom, poisson, chi2\n",
    "from IPython.display import clear_output\n",
    "from operator import itemgetter\n",
    "from statsmodels.stats import proportion\n",
    "from scipy.special import comb\n",
    "\n",
    "from numpy import matlib\n",
    "\n",
    "# Parameters\n",
    "n = 10      # number of quanta\n",
    "p = 0.2     # release probability\n",
    "\n",
    "# Compute binomial probabilities\n",
    "## k is the number of quanta released\n",
    "## comb is n-choose-k\n",
    "probabilities = []      # list to store probabilities\n",
    "for k in range(n+1):\n",
    "    Pk = comb(n, k) * (p**k) * ((1-p)**(n-k)) # binomial probability\n",
    "    probabilities.append(Pk)        # store probability\n",
    "    print(f\"{k} quanta released: {Pk:.10f} ({Pk*100:.4f}%)\")    # print probability for k quanta -- what is .10f and .4f?\n",
    "\n",
    "# Check that total probability sums to 1\n",
    "print(f\"\\nSum of probabilities = {sum(probabilities):.10f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebdb5eb",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Let's say you know that a given nerve terminal contains exactly 14 quanta available for release. You have read in the literature that the release probability of these quanta is low, say 0.1. To assess whether this value is reasonable, you run a simple experiment: activate the nerve and measure the number of quanta that are released. The result is 8 quanta. What is the probability that you would get this result (8 quanta) if the true probability of release really was 0.1? What about if the true release probability was much higher; say, 0.7? What about for each decile of release probability (0.1, 0.2, ... 1.0)? Which value of release probability did you determine to be the most probable, given your measurement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1bbfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p=0.1 -> P(observe 8) = 1.59592e-05\n",
      "p=0.2 -> P(observe 8) = 2.01528e-03\n",
      "p=0.3 -> P(observe 8) = 2.31800e-02\n",
      "p=0.4 -> P(observe 8) = 9.18212e-02\n",
      "p=0.5 -> P(observe 8) = 1.83289e-01\n",
      "p=0.6 -> P(observe 8) = 2.06598e-01\n",
      "p=0.7 -> P(observe 8) = 1.26202e-01\n",
      "p=0.8 -> P(observe 8) = 3.22445e-02\n",
      "p=0.9 -> P(observe 8) = 1.29269e-03\n",
      "p=1.0 -> P(observe 8) = 0.00000e+00\n"
     ]
    }
   ],
   "source": [
    "from math import comb\n",
    "\n",
    "n = 14   # quanta available\n",
    "k = 8    # observed released\n",
    "\n",
    "# Compute for each decile\n",
    "for p in [i/10 for i in range(1, 11)]:                  # p from 0.1 to 1.0\n",
    "    P = comb(n, k) * (p**k) * ((1-p)**(n-k))            # binomial probability\n",
    "    print(f\"p={p:.1f} -> P(observe {k}) = {P:.5e}\")     # print probability\n",
    "\n",
    "# given these measurements, the most probable release probability is 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15c2bd",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "## part 1\n",
    "Not feeling convinced by your single experiment (good scientist!), you repeat it under identical conditions. This time you measure 5 quanta that were released. Your sample size has now doubled, to two measurements. You now want to take into account both measurements when you assess the likelihoods of different possible values of the underlying release probability. To do so, assume that the two measurements in this sample are independent of one another; that is, the value of each result had no bearing on the other. In this case, the total likelihood is simply the product of the likelihoods associated with each separate measurement. It is also typical to compute the logarithm of each likelihood and take their sum, which is often more convenient. What are the values of the total likelihood and total log-likelihood in this example, if we assume that the true release probability is 0.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad17698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(k=8 | p=0.1) = 1.59592e-05\n",
      "P(k=5 | p=0.1) = 7.75616e-03\n",
      "\n",
      "Total likelihood = 1.23782e-07\n",
      "Total log-likelihood = -15.905\n"
     ]
    }
   ],
   "source": [
    "from math import comb, log\n",
    "\n",
    "# Parameters\n",
    "n = 14                # number of quanta available\n",
    "measurements = [8, 5] # observed releases\n",
    "p = 0.1               # assumed release probability\n",
    "\n",
    "# Compute likelihoods for each measurement\n",
    "likelihoods = []                                     # list to store likelihoods\n",
    "for k in measurements:                               # for each observed k\n",
    "    Pk = comb(n, k) * (p**k) * ((1-p)**(n-k))        # binomial probability\n",
    "    likelihoods.append(Pk)                           # store likelihood\n",
    "    print(f\"P(k={k} | p={p}) = {Pk:.5e}\")            # print likelihood\n",
    "\n",
    "# Total likelihood (product)\n",
    "L_total = 1                      # initialize total likelihood\n",
    "for val in likelihoods:     # for each likelihood value\n",
    "    L_total *= val           # multiply to total -- what is *= ?\n",
    "\n",
    "# Total log-likelihood (sum of logs)\n",
    "logL_total = sum(log(val) for val in likelihoods)   # sum of log-likelihoods\n",
    "\n",
    "print(f\"\\nTotal likelihood = {L_total:.5e}\")\n",
    "print(f\"Total log-likelihood = {logL_total:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eea382",
   "metadata": {},
   "source": [
    "## part 2\n",
    "Of course, knowing those values of the likelihood and log-likelihood is not particularly useful until you can compare them to the values computed for other possible values for the release probability, so you can determine which value of release probability is most likely, given the data. Therefore, compute the full likelihood and log-likelihood functions using deciles of release probability between 0 and 1. What is the maximum value? Can you improve your estimate by computing the functions at a higher resolution? How does the estimate improve as you increase the sample size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e4db37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   p      Likelihood       Log-likelihood\n",
      "---------------------------------------------\n",
      "0.1     1.23782e-07              -15.905\n",
      "0.2     1.73284e-04               -8.661\n",
      "0.3     4.55058e-03               -5.393\n",
      "0.4     1.89700e-02               -3.965\n",
      "0.5     2.23965e-02               -3.799\n",
      "0.6     8.43113e-03               -4.776\n",
      "0.7     8.35820e-04               -7.087\n",
      "0.8     1.08303e-05              -11.433\n",
      "0.9     1.52817e-09              -20.299\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         L_total *= Pk\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# compute log-likelihood\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     logL_total = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m-\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m-\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmeasurements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>3.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m15.5e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogL_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m20.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# what is meant by \"maximum value\"? maximum value of what??\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# what is meant by higher resolution?\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     16\u001b[39m         L_total *= Pk\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# compute log-likelihood\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     logL_total = \u001b[38;5;28msum\u001b[39m(\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m-\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m-\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m measurements)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>3.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m15.5e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogL_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m20.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# what is meant by \"maximum value\"? maximum value of what??\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# what is meant by higher resolution?\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: math domain error"
     ]
    }
   ],
   "source": [
    "from math import comb, log\n",
    "\n",
    "# Parameters\n",
    "n = 14                # number of quanta available\n",
    "measurements = [8, 5] # observed releases\n",
    "ps = [i/10 for i in range(1, 11)]  # p = 0.1, 0.2, ..., 1.0\n",
    "\n",
    "print(f\"{'p':>4} {'Likelihood':>15} {'Log-likelihood':>20}\")\n",
    "print(\"-\"*45)\n",
    "\n",
    "for p in ps:\n",
    "    # compute product of likelihoods across measurements\n",
    "    L_total = 1\n",
    "    for k in measurements:\n",
    "        Pk = comb(n, k) * (p**k) * ((1-p)**(n-k))\n",
    "        L_total *= Pk\n",
    "\n",
    "    # compute log-likelihood\n",
    "    logL_total = sum(log(comb(n, k) * (p**k) * ((1-p)**(n-k))) for k in measurements)\n",
    "\n",
    "    print(f\"{p:>3.1f} {L_total:15.5e} {logL_total:20.3f}\")\n",
    "\n",
    "\n",
    "# what is meant by \"maximum value\"? maximum value of what??\n",
    "# what is meant by higher resolution?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740bed5",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "You keep going and conduct 100 separate experiments and end up with these results:\n",
    "\n",
    "\n",
    "What is the most likely value of \"p-hat\" (represents the maximum-likelihood estimate of a parameter in the population given our sample) with a resolution of 0.01?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bca60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using supplied counts:\n",
      " total trials = 107\n",
      " total successes = 653\n",
      " analytic MLE p_hat = 0.435915\n",
      " grid-search best p (0.01 resolution) = 0.44 with log-likelihood = -218.593512\n"
     ]
    }
   ],
   "source": [
    "# counts as provided (k: count)\n",
    "counts = {0:0,1:0,2:3,3:7,4:10,5:19,6:26,7:16,8:16,9:5,10:5,11:0,12:0,13:0,14:0}\n",
    "\n",
    "from math import comb, log\n",
    "\n",
    "n = 14                                                           # number of quanta available\n",
    "total_trials = sum(counts.values())                              # total number of trials\n",
    "total_successes = sum(k * c for k, c in counts.items())          # total number of successes\n",
    "\n",
    "# analytic MLE\n",
    "p_hat_analytic = total_successes / (n * total_trials)            # analytic MLE for p\n",
    "\n",
    "# grid search at 0.01 resolution to find best p\n",
    "ps = [i / 100 for i in range(0, 101)]\n",
    "best_p = None\n",
    "best_loglik = -1e300\n",
    "for p in ps:\n",
    "    # handle p==0 or p==1 safely\n",
    "    if p == 0.0:\n",
    "        # if any observed k>0, log-likelihood is -inf (we'll treat as a very large negative)\n",
    "        if any(k > 0 and counts.get(k, 0) > 0 for k in range(n+1)):\n",
    "            loglik = -1e300\n",
    "        else:\n",
    "            loglik = 0.0\n",
    "    elif p == 1.0:\n",
    "        if any(k < n and counts.get(k, 0) > 0 for k in range(n+1)):\n",
    "            loglik = -1e300\n",
    "        else:\n",
    "            loglik = 0.0\n",
    "    else:\n",
    "        loglik = 0.0\n",
    "        for k, c in counts.items():\n",
    "            if c == 0:\n",
    "                continue\n",
    "            loglik += c * (log(comb(n, k)) + k * log(p) + (n - k) * log(1 - p))\n",
    "\n",
    "    if loglik > best_loglik:\n",
    "        best_loglik = loglik\n",
    "        best_p = p\n",
    "\n",
    "print(\"Using supplied counts:\")\n",
    "print(f\" total trials = {total_trials}\")\n",
    "print(f\" total successes = {total_successes}\")\n",
    "print(f\" analytic MLE p_hat = {p_hat_analytic:.6f}\")\n",
    "print(f\" grid-search best p (0.01 resolution) = {best_p:.2f} with log-likelihood = {best_loglik:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a657483d",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "## part 1\n",
    "Let's say that you have run an exhaustive set of experiments on this synapse and have determined that the true release probability is 0.3 (within some very small tolerance). Now you want to test whether changing the temperature of the preparation affects the release probability. So you change the temperature, perform the experiment, and measure 7 quantal events for the same 14 available quanta. Compute p-hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "783b165f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-hat = 0.50\n",
      "Standard error = 0.134\n",
      "Approx 95% CI = (0.24, 0.76)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# parameters\n",
    "n = 14   # total quanta available\n",
    "k = 7    # observed released\n",
    "\n",
    "# compute MLE (p-hat)\n",
    "p_hat = k / n\n",
    "\n",
    "# compute standard error\n",
    "se = math.sqrt(p_hat * (1 - p_hat) / n)\n",
    "\n",
    "# 95% confidence interval (Wald)\n",
    "ci_low = p_hat - 1.96 * se\n",
    "ci_high = p_hat + 1.96 * se\n",
    "\n",
    "print(f\"p-hat = {p_hat:.2f}\")\n",
    "print(f\"Standard error = {se:.3f}\")\n",
    "print(f\"Approx 95% CI = ({ci_low:.2f}, {ci_high:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3a334",
   "metadata": {},
   "source": [
    "## part 2\n",
    "Standard statistical inference now asks the question, what is the probability that you would have obtained that measurement given a Null Hypothesis of no effect? In this case, no effect corresponds to an unchanged value of the true release probability (i.e., its value remained at 0.3 even with the temperature change). What is the probability that you would have gotten that measurement if your Null Hypothesis were true? Can you conclude that temperature had an effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2765e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X >= 7) under H0 = 0.09328\n",
      "P(X <= 1) under H0 = 0.04748\n",
      "Two-sided p-value = 0.14076\n"
     ]
    }
   ],
   "source": [
    "from math import comb\n",
    "\n",
    "n = 14\n",
    "p0 = 0.3\n",
    "observed = 7\n",
    "\n",
    "# probability mass function\n",
    "def binom_pmf(k, n, p):\n",
    "    return comb(n, k) * (p**k) * ((1-p)**(n-k))\n",
    "\n",
    "# compute upper tail P(X >= observed)\n",
    "p_upper = sum(binom_pmf(k, n, p0) for k in range(observed, n+1))\n",
    "\n",
    "# find symmetric lower cutoff to determine P(X <= lower_cutoff)\n",
    "expected = n * p0\n",
    "distance = observed - expected\n",
    "lower_cutoff = int(expected - distance)  # floor\n",
    "p_lower = sum(binom_pmf(k, n, p0) for k in range(0, lower_cutoff+1))\n",
    "\n",
    "p_value = p_upper + p_lower\n",
    "\n",
    "print(f\"P(X >= {observed}) under H0 = {p_upper:.5f}\")\n",
    "print(f\"P(X <= {lower_cutoff}) under H0 = {p_lower:.5f}\")\n",
    "print(f\"Two-sided p-value = {p_value:.5f}\")\n",
    "\n",
    "# conclusion: two-sided p-value > 0.05, so we do not reject H0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
